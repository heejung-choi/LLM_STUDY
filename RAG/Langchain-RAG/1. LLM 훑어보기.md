# 1. LLM 훑어보기
## 1.1 생성 AI의 열풍의 주역 LLM

### 규칙 기반 자연어 처리, 최초 AI 챗봇 ELIZA
- AI 챗봇의 시초
- 사용자가 입력한 문장에서 키워드를 인식하고 찾아낸 키워드를 바탕으로 정해진 규칙에 따라 사용자의 입력을 변환하고 이를 응답 생성에 활용한다.
- 일라이자 효과: 인간이 컴퓨터의 행동에 지나치게 의미를 부여하고 무의식적으로 의인화하는 현상
- 규칙기반, 자연어처리 알고리즘

### 자연어 처리(NLP)
인간의 언어를 해석, 조작, 이해하는 능력을 컴퓨터에 부여하는 기계학습 기술
- 자연어 처리 모델: 통계적 추론, 머신러닝, 딥러닝 등의 기계학습 기법에 기반하여 자연어 처리를 위해 학습된 기계학습 모델
- 대표적 통계적 자연어 처리 모델: N-gram, TF-IDF
- 대표적 머신러닝 자연어 처리 모델: BERT, GPT

### N-gram
- 문장을 n개의 단어로 쪼개어 단어의 연속성을 파악하는 통계적 언어 모델
- HMM: 이전 상태를 기반으로 다음 상태를 예측하는 마르코프 모델에 은닉상태를 더한 것으로 연속적인 데이터를 처리하는데 강점을 지닌 모델
- CRP: HMM과 유사하게 연속적인 데이터를 처리하지만 각 상태가 이전 상태 뿐만 아니라 다른 관측치에도 의존할 수 있게 만드는 조건부 확률 모델
- HMM, CRP의 한계: 희소성 문제 해결이 어렵고, 문맥을 파악하기 어렵다는 점(너무 많은 경우의 수) -> 이를 해결하기 위해 N-gram 모델이 등장
- N-gram 모델: 문장을 n개의 단어로 쪼개어 단어의 연속성을 파악하는 통계적 언어 모델로, 문장을 n개의 단어로 쪼개어 단어의 연속성을 파악하는 통계적 언어 모델
- N-gram 모델의 한계: 모든 단계를 고려하는 것이 아니기 때문에 예측 정확도가 떨어질 수 있다.

### 딥러닝
- 사람의 뇌를 모방한 인공신경망을 여러층 쌓아 컴퓨터가 데이터를 학습하는 기술

### CNN
- 이미지 인식 분야에서 주로 사용되는 딥러닝 모델로, 이미지의 특징을 추출하는데 강점을 지닌다.
- 한계점: 접근방식의 한계, 문맥의 상호관계 어려움, 텍스트 데이터의 순서를 무시한다.
- 이러한 한계를 극복하고자 RNN 모델이 NLP에 접목되기 시작함

### RNN
- 연속된 데이터의 각 요소를 순차적으로 처리하면서 이전 단계의 정보를 다음 단계에 전달한다는 특징이 있음
- 이전 데이터를 기억할 수 있는 첫 번째 알고리즘으로 각 노드가 다음 노드들과 연결되는 순환 구조를 지녔으며, 각 노드가 과거의 정보를 기억하고 있음
- RNN의 한계: 기울기 소실 문제 (이전 노드가 갖고 있는 정보를 은닉 상태로써 다음 노드에 전달하기 위해서는 신경망의 활성화 함수를 통과해야 하는데, 이 과정에서 기울기가 소실되는 문제가 발생)
- 이러한 한계를 극복하기 위해 LSTM 모델 등장

### LSTM
- RNN의 기울기 소실 문제를 해결하기 위해 고안된 모델로, RNN의 장기 의존성 문제를 해결하기 위해 고안됨
- LSTM의 구조: 입력 게이트, 망각 게이트, 출력 게이트로 구성된 셀로 이루어져 있으며, 각 게이트는 입력, 출력, 삭제의 역할을 수행

### 어텐션 메커니즘
#### 인코더-디코더 아키텍처 
- 인코더가 입력 문장을 은닉상태로 변환하고 다시 디코더가 출력문장으로 변환하는 작업을 수행한다. 
- 여기서 번역 성능은 인코더에서 받아들인 입력 문장의 정보를 디코더가 잘 활용할 수 있는지에 달려있는데, 이를 위해 어텐션 메커니즘이 등장
 * 은닉상태(Hidden State)는 인공신경망, 특히 순환신경망(RNN)과 그 변형 모델들(LSTM, GRU)에서 중요한 개념입니다. 은닉상태는 네트워크가 입력 데이터를 처리하면서 내부적으로 유지하는 상태를 의미합니다. 이는 네트워크가 이전 시점의 정보를 기억하고, 이를 바탕으로 현재 시점의 출력을 생성하는 데 사용됩니다.
- 어텐션 메커니즘: 디코더가 출력 단어를 생성하는 매 시점마다 인코더의 전체 입력 문장을 다시 한 번 참고하여 단어를 예측하는 방법
- multi-head attention: 여러 개의 어텐션을 병렬로 수행하여 각 어텐션의 결과를 합치는 방식

## 1.2 LLM 개발의 양대산맥, 오픈소스 LLM과 Closed LLM
### 오픈소스 LLM의 장점
1) 사전학습 비용을 감당하지 않아도 된다.
2) 자유로운 커스터마이징이 가능하다. 
OpenAI에서도 GPT 모델을 파인튜닝하여 API로 활용할 수 있는 기능을 제공하지만, 오픈소스 LLM의 파인튜닝과는 거리가 있다.
OpenAI의 GPT 모델을 파인튜닝하는 경우, 모델의 구조를 정확히 알 수 없도 성능향상이 어렵지만 오픈소스 LLM의 경우 모델을 구성하는 레이어들의 가중치를 공개하고 있어 성능향상이 가능하다.
3) 비용, 성능, 보안 측면에서 많은 차이점을 지닌다.

## 1.3 LLM의 활용 현황
![ai시장지도](https://cdn.maily.so/oyf8nautaueamlkssv8muxdkb6hq)

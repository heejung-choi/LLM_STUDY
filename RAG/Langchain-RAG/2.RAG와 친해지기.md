# 2. RAG와 친해지기
## 2.1 RAG 시스템이란?

### LLM 최대의 약점, 환각현상
- LLM은 트랜스포머의 디코더 모듈이 여러 개 결합된 모델로, 텍스트를 생성하는 기능에 특화되어 있다. 
  - 즉, 어떤 문장이 주어지도록 답변하도록 훈력이 되어있는데 이런 특성으로 인해 가끔 사실이 아닌 것을 사실인 양 자연스럽게 답변하며 거짓정보를 내뱉는 경우가 있다.
  - 이러한 현상을 환각현상이라고 하는데, 이는 LLM의 가장 큰 약점 중 하나이다.

### context window
- context window 란 입력으로 주어진 텍스트 값으 길이를 뜻하는 것으로 LLM은 어떤 모델이든지 입력 값 길이의 상한선이 존재한다.
- LLM은 주어진 텍스트를 이해하기 위해 행렬 연산을 수행하는데, 이 과정에서 입력 값이 길면 행렬의 크기가 커지고 이에 따라 계산량은 기하급수적으로 늘어나게 된다.
- 결과적으로 문장의 길이가 길어질수록 계산량이 많아지고 이를 저장할 메모리 공간이 더 많이 필요한 것이다.

## 환각 현상을 극복한 RAG
- RAG는 Retrieval Augmented Generation의 약자로, 검색 증강 생성 기법이라고도 한다.
- 검색 - 증강 - 생성의 3단계를 통하여 LLM이 사실에 근거한 답변을 하도록 만든다.
- RAG 시스템은 사용자 질문에 답변하기 위해 LLM에만 의존하는 것이 아니라 외부 지식을 참고하도록 만든다.

### 1. 검색(Retrieval)
- RAG의 본질은 LLM이 올바른 정보 출처를 찾아 이를 기반으로 환각 현상을 해결하는 것이다.
- RAG는 기본적으로 사용자의 질문과 가장 유사한 정보를 찾기 위해 임베딩 값의 유사도를 이용한다.
1) 사용자의 질문을 행렬 값(임베딩)으로 변환한다.
이 작업을 수행하기 위해 일반적으로는 BERT와 그 파생 모델들을 활용한다.
2) 참고핧 문서들을 동일한 방식을 통해 행렬 값으로 변환한다. -> 이를 벡터 DB라고 부른다.
3) 벡터 DB에서 사용자의 질문 임베딩과 가장 유사한 임베딩을 추출하여 해당 임베딩의 문장을 추춣한다. -> 이를 검색이라고 한다.

- 검색 단계에서 주목해야 할 두가지는 임베딩 모델과 검색하여 가져올 문장의 형태이다.
1) 임베딩 모델
사용자의 질문과 참고 문서를 얼마나 임베딩 값으로 잘 변환하느냐를 기준으로 좋은 성능을 지닌 임베딩 모델을 활용해야 한다.
2) 검색으로 가져올 문장의 형태
만약, 조선왕조실록을 벡터 DB로 구축한다고 가정했을 때 6400만 자의 글을 벡터 DB를 통해 하나의 임베딩 값으로 변환할 수 있을까?
임베딩 모델도 LLM과 마찬가지로 컨텍스트 윈도우라는 개념이 존재하고 제한된 값을 가지므로 불가능하다.
따라서 벡터 DB를 구축할 때는 우리가 가진 문서가 임베딩 모델의 최대 입력 길이를 벗어날 경우 문서를 분할하는 과정을 거쳐야 한다.
-> 이 과정을 Chunking(조각 내기)라고 표현한다.
### chunking(조각 내기)
- 검색에 있어 chunking은 매우 중요하다. 
- 사용자의 질문 임베딩과 벡터 DB 내의 임베딩 값들을 비교 하여 가장 유사한 임베딩 값을 찾아냈다고 가정했을 때, 적절한 정보를 잘 찾았더라도 찾은 문서 덩어리에는 사용자 질문에 답변할 수 있는 맥락이 충분히 담기지 않았을 수 있다.
- 예를 들어 단순히 임베딩 모델의 최대 입력 길이 만큼 문서를 분할 했을 경우, 유사 문서 청크에 온전한 문장이 아닌 중간에 잘린 문장이 있을 수 있고, 이로 예기치 않은 환각 현상을 불러 일으킬 수 있다.

** 환각현상(Hallucination)은 대형 언어 모델(LLM)이 사실이 아닌 정보를 생성하는 현상을 말한다. 이는 모델이 문맥을 잘못 이해하거나, 학습 데이터에 없는 정보를 생성할 때 발생한다.
- 따라서 문서를 분할하여 임베딩 값으로 변환하는 과정에서 어떤 방식으로 분할할지, 또 어떤 구조로 문서를 분할하여 저장할지 고민하는 것은 매우 신중해야 한다.

### 2. 증강(Augment)
- 증강 단계는 사용자 질문 프롬포트와 검색 단계에서 추출한 유사 청크를 하나의 프롬포트로 담아 LLM에게 전달하며 비교적 간단하게 동작한다.

### 3. 생성(Generation)
- LLM에게 아무런 맥락이 주어지지 않은 프롬포트를 입력하면 사전 학습시 습득한 지식을 바탕으로 답변을 생성하는데 이때 사전 학습에서 배우지 못한 개념이나 사실들은 환각현상을 일으키기 쉽다.
- 그러나 RAG에서는 앞서 증강단계에서 주어진 프롬포트를 바탕으로 LLM이 답변을 생성한다.
- 이렇게 프롬포트 안에 context를 주입하면, 기존에 학습되지 않은 사실이나 지식에 대해 LLM이 이해하고 알맞는 답변을 제공할 수 있다.

## 2.2 RAG VS 파인튜닝
### 파인튜닝
- 파인튜닝은 모델을 미세조정한다는 개념으로, 기존에 LLM이 사전 학습과정에서 습득한 지식 외의 데이터 셋을 기반으로 재조정하여 새로운 지식 및 답변 방식을 훈련시키는 것이다.
- ex) Bloomberg-GPT : 블룸버그라는 기업에서 금융분야 데이터셋을 기반으로 LLM을 파인튜닝한 것으로 기존 LLM 대비 금융 관련 질문에 더 진실되고 명확한 답변을 얻을 수 있다.

|             | RAG                   | 파인튜닝                          |
|-------------|-----------------------|-------------------------------|
| 비용          | 저가                    | 고가                            |
| 시간          | 단기간 구축 가능             | 학습 위해 장기간 소요                  |
| 난이도         | 쉬움                    | 어려움                           |
| 필수<br/>하드웨어 | 임베딩 모델과 LLM 구동 위한 GPU | 파인튜닝이 가능할 정도의 높은 VRAM을 가진 GPU |